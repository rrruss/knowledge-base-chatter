# -*- coding: utf-8 -*-
"""scraping_bs.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12jHMlZPZceujMCH8j_Bu90UKjFO0c_to
"""

from bs4 import BeautifulSoup
import requests
import pandas as pd
from datetime import datetime
import urllib
import re

def prepare_main_QA_df(soup,Tip):
  Questions = []
  for heading in soup.find_all(["h2"]):
    Questions.append(heading.text.strip())
  answers = []
  college_name_list = soup.find_all('h2')
  for college in college_name_list:
     nextNode = college
     curr_answer = ""
     while True:
        nextNode = nextNode.find_next_sibling()
        if nextNode and nextNode.name != 'h1' and nextNode.name != 'h2'  and  nextNode.name != 'h3':
  #          print (nextNode.text)
             curr_answer = curr_answer + nextNode.text.strip()
        else:
             answers.append(curr_answer)
             break
  questions = Questions[:-2]
  answers = answers[:-2]
  questions_final = []
  context_str = ""
  str1 = " "
  answer_start = []
  for question in questions:
     questions_final.append(question.split()[0] + ' ' + Tip + ' ' + str1.join(question.replace('it','').split()[1:]))
  for i,questions in enumerate(questions_final):
    answer_start.append(len(context_str + questions.replace(':','')))
    context_str = context_str + questions.replace(':','') + ':' + answers[i] + '. '
  context = [context_str]*len(questions_final)
  df_main = pd.DataFrame(list(zip(questions_final, answers,context,answer_start)),
               columns =['Question', 'Answer', 'Context', 'answer_start'])
  return df_main

def prepare_aside_QA_df(soup,Tip):
      idx = 0
      questions = []
      answers = []
      for strong_tag in soup.find_all('strong'):
   #     print("T",strong_tag.text)
        if (idx %2 == 0):
          questions.append(strong_tag.text)
        else:
          answers.append(strong_tag.text)
        idx = idx + 1
        if strong_tag.next_sibling is not None and strong_tag.next_sibling.name != 'br':
   #       print("TT", strong_tag.next_sibling)
          if (idx %2 == 0):
            questions.append(strong_tag.text)
          if (idx % 2 == 1):
            answers.append(strong_tag.next_sibling)
          idx = idx + 1
        if strong_tag.next_sibling is not None and strong_tag.next_sibling.next_sibling is not None:
   #       print("TTT", strong_tag.next_sibling.next_sibling)
          if (idx % 2 == 1):
            answers.append(strong_tag.next_sibling.next_sibling)
          idx = idx + 1
  #    print("Questions", questions)
  #    print("answers", answers)
      questions = questions[:-2]
      answers = answers[:-1]
      questions_final = []
      answers_final = []
      context_str = ""
      answer_start = []
      for question in questions:
            questions_final.append(Tip + ' ' + question.replace('This', ''))
      for i,questions in enumerate(questions_final):
            answer_start.append(len(context_str + questions.replace(':','')))
            context_str = context_str + questions.replace(':','') + ':' + answers[i] + '. '
 #           print(context_str)
      context = [context_str]*len(questions_final)
      df_aside = pd.DataFrame(list(zip(questions_final, answers,context,answer_start)),
               columns =['Question', 'Answer', 'Context', 'answer_start'])
      return df_aside

def extract_tip(soup):
  for heading in soup.find_all(["h1"]):
#    print(heading.text.strip())
#    len = len + 1
     return heading.text.strip()


url = 'https://slack.com/slack-tips'

response = requests.get(url)

page = response.text
soup = BeautifulSoup(page, "lxml")

data = soup.findAll('div',attrs={'class':'slacktips-homepage-bg'})

column_names = ["Question", "Answer", "Context", "answer_start"]
df = pd.DataFrame(columns = column_names)
count = 0
save_list = []
for div in data:
    links = div.findAll('a')
    for a in links:
          url = a['href']
          response = requests.get(url)
          page = response.text
          soup = BeautifulSoup(page, "lxml")
          found = False
          for strong_tag in soup.find_all('strong'):
           if strong_tag.text == "This tip uses:":
             found = True
             break
          if found:
             Tip = extract_tip(soup)
             df_aside = prepare_aside_QA_df(soup,Tip)
             df_main = prepare_main_QA_df(soup,Tip)
             frames = [df, df_main, df_aside]
             df = pd.concat(frames,ignore_index=True)
          else:
             save_list.append(a['href'])


column_names = ["Question", "Answer", "Context", "answer_start"]

df_slack = pd.DataFrame(columns = column_names)

count = 0
for url in save_list:
  response = requests.get(url)
  page = response.text
  soup = BeautifulSoup(page, "lxml")
  data = soup.findAll('div',attrs={'class':'o-content-container'})
  for div in data:
    links = div.findAll('a')
    for a in links:
      if "https://slack.com/slack-tips" in a['href']:
         url_t = a['href']
         response = requests.get(url_t)
         page = response.text
         soup = BeautifulSoup(page, "lxml")
         found = False
         for strong_tag in soup.find_all('strong'):
           if strong_tag.text == "This tip uses:":
              found = True
              break
         if found:
           Tip = extract_tip(soup)
           df_aside = prepare_aside_QA_df(soup,Tip)
           df_main = prepare_main_QA_df(soup,Tip)
           frames = [df_slack, df_main, df_aside]
           df_slack = pd.concat(frames,ignore_index=True)

frames = [df, df_slack]
df_comb = pd.concat(frames,ignore_index=True)

df_comb.drop_duplicates(ignore_index=True)

df_comb.to_csv('slack_tips_qa_nodup_answer_start.csv',index=False)
